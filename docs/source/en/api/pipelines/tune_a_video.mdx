<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Tune-A-Video

## Overview

[Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation](https://arxiv.org/abs/2212.11565) by Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou
The abstract of the paper is the following:

*To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation settingâ€”One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.*

Resources:

* [GitHub repository](https://github.com/showlab/Tune-A-Video)
* [ðŸ¤— Spaces](https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI)

## Available Pipelines:

| Pipeline | Tasks | Demo
|---|---|:---:|
| [TuneAVideoPipeline](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/tune_a_video//Users/nagasaiabhinaydevarinti/Desktop/diffusers/src/diffusers/pipelines/tune_a_video/pipeline_tune_a_video.py) | *Text-to-Video Generation* | [ðŸ¤— Spaces](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)

## Usage example 

```python
import torch
from diffusers import DiffusionPipeline, DDIMScheduler
from diffusers.utils import export_to_video
from PIL import Image


pretrained_model_path = "nitrosocke/mo-di-diffusion"

pipe = TuneAVideoPipeline.from_pretrained("NagaSaiAbhinay/tune-a-video-mo-di-bear-guitar-v1", torch_dtype=torch.float16).to("cuda")

prompt = "A princess playing a guitar, modern disney style"
generator = torch.Generator(device="cuda").manual_seed(42)

video_frames = pipe(prompt, video_length=3, generator=generator, num_inference_steps=50, output_type="np").frames

#Saving to gif.
pil_frames = [Image.fromarray(frame) for frame in video_frames]
duration = len(pil_frames) / 8
pil_frames[0].save(
        'animation.gif',
        save_all=True,
        append_images=pil_frames[1:], # append rest of the images
        duration=duration * 1000, # in milliseconds
        loop=0)

#Saving to video
video_path = export_to_video(video_frames)

```

Here are some sample outputs: 

<table>
    <tr>
        <td><center>
        A princess playing a guitar, modern disney style
        <br>
        <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astr.gif"
            alt="A princess playing a guitar, modern disney style"
            style="width: 300px;" />
        </center></td>
    </tr>
</table>

## Available checkpoints 

* [NagaSaiAbhinay/tune-a-video-mo-di-bear-guitar-v1](https://huggingface.co/NagaSaiAbhinay/tune-a-video-mo-di-bear-guitar-v1)

## TuneAVideoPipeline
[[autodoc]] TuneAVideoPipeline
	- all
	- __call__
